{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0442bfe6-159d-41a1-a457-b48500b0fff5",
   "metadata": {},
   "source": [
    "# üîç Deep Learning\n",
    "Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems. It's inspired by the way the human brain works ‚Äî with layers of neurons that process information.\n",
    "\n",
    "## Common Deep Learning Frameworks:\n",
    "- TensorFlow (Google)\n",
    "- PyTorch (Facebook/Meta)\n",
    "- Keras (High-level API for TensorFlow)\n",
    "\n",
    "---\n",
    "# üîç Neural Network\n",
    "A Neural Network is a computational model inspired by the human brain. It consists of neurons (also called nodes) arranged in layers that work together to learn patterns from data.\n",
    "\n",
    "Each neuron:\n",
    "- Takes input\n",
    "- Applies a weight and bias\n",
    "- Passes it through an activation function\n",
    "- Produces output\n",
    "These outputs then feed into the next layer of neurons ‚Äî like a pipeline.\n",
    "\n",
    "<img src=\"https://www.simplyblock.io/wp-content/media/a7fbb2_553adcbda4b346b28831a3f94d2994camv2.jpg\" alt=\"A beautiful sunset\" width=\"500\" />\n",
    "\n",
    "---\n",
    "## üß© Types of Neural Networks\n",
    "\n",
    "There are several types, each good for different tasks:\n",
    "\n",
    "| Type                                | Description                                                             | Use Case                                              |\n",
    "|-------------------------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| **1. Feedforward Neural Network (FNN)** | Simple network where data flows one way (input ‚Üí output).              | Basic classification and regression tasks.             |\n",
    "| **2. Convolutional Neural Network (CNN)** | Uses filters to detect patterns in images.                             | Image classification, object detection, facial recognition. |\n",
    "| **3. Recurrent Neural Network (RNN)** | Has loops ‚Äî output from one step is input to the next.                 | Time series, speech recognition, text generation.      |\n",
    "| **4. Long Short-Term Memory (LSTM)** | A type of RNN that solves memory issues.                               | Language modeling, chatbots, translation.              |\n",
    "| **5. Generative Adversarial Network (GAN)** | Two networks (Generator & Discriminator) compete to create realistic data. | Image generation, deepfakes, art generation.           |\n",
    "| **6. Autoencoder**                   | Learns to compress and reconstruct data.                               | Noise reduction, anomaly detection, dimensionality reduction. |\n",
    "| **7. Transformer**                  | Works on sequences but in parallel (no recurrence).                    | Language models like ChatGPT, BERT, translation.       |\n",
    "\n",
    "\n",
    "---\n",
    "# üîç Activation Functions in Deep Learning\n",
    "An activation function is a mathematical function used in neural networks to determine whether a neuron should be \"activated\" or not ‚Äî i.e., it decides whether the information that the neuron is processing is important enough to pass to the next layer.\n",
    "\n",
    "## üß† Popular Activation Functions\n",
    "\n",
    "| Activation Function     | Formula / Behavior                            | Use Case / Notes                                                                 |\n",
    "|-------------------------|-----------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| **Sigmoid**             | $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $        | Output between 0 and 1. Good for binary classification. Can cause vanishing gradients. |\n",
    "| **Tanh**                | $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $ | Output between -1 and 1. Better than sigmoid, but still has vanishing gradient issues. |\n",
    "| **ReLU (Rectified Linear Unit)** | $ f(x) = \\max(0, x) $                 | Most common. Fast and simple. Good for hidden layers. Can suffer from \"dead neurons.\" |\n",
    "| **Leaky ReLU**          | $ f(x) = \\max(0.01x, x) $                   | Fixes ReLU‚Äôs dead neuron issue with a small slope for negative values.          |\n",
    "| **Softmax**             | $ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum e^{x_j}} $ | Converts outputs to probabilities. Used in the output layer for multi-class classification. |\n",
    "| **Swish / GELU / ELU**  | Various modern functions                      | Often used in advanced models like transformers. Smoother, better performance in some cases. |\n",
    "\n",
    "---\n",
    "## üîÅ Step-by-Step: How Weights & Biases are Updated\n",
    "\n",
    "### ‚öôÔ∏è 1. Forward Propagation\n",
    "The input data goes through the network layer by layer.\n",
    "\n",
    "At each neuron:\n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b\n",
    "$$\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n",
    "\n",
    "- `w`: weight  \n",
    "- `x`: input  \n",
    "- `b`: bias  \n",
    "- `f(z)`: activation function output\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ 2. Loss Calculation\n",
    "- Compare the predicted output with the true label using a **loss function** (e.g., MSE, Cross-Entropy).\n",
    "- This gives us a **loss value** representing the error.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ 3. Backpropagation\n",
    "- The error (loss) is **propagated backward** through the network.\n",
    "- We calculate the **gradient (slope)** of the loss with respect to each weight and bias using the **chain rule** from calculus.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Update Rule: Gradient Descent\n",
    "\n",
    "Weights and biases are updated using the **Gradient Descent** algorithm:\n",
    "\n",
    "$$\n",
    "w := w - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "$$\n",
    "b := b - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `w` = weight  \n",
    "- `b` = bias  \n",
    "- `Œ∑` = learning rate  \n",
    "- `‚àÇLoss/‚àÇw`, `‚àÇLoss/‚àÇb` = gradients\n",
    "\n",
    "These gradients tell us how much a small change in weight or bias will affect the loss.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ This process repeats for many epochs:\n",
    "- **Forward pass** ‚Üí compute output  \n",
    "- **Compute loss**  \n",
    "- **Backward pass** ‚Üí compute gradients  \n",
    "- **Update** weights & biases using gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80559d39-8821-49d8-b2c7-43f3050ba2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
