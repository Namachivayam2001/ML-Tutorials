{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0138be-e409-4ce9-92f3-d31354ed8ba3",
   "metadata": {},
   "source": [
    "## 🔎 Dimensionality Reduction\n",
    "Dimensionality reduction is a process used in data analysis and machine learning to reduce the number of input variables (or features) in a dataset while retaining as much important information (variance or structure) as possible.\n",
    "\n",
    "---\n",
    "## 🔎 PCA\n",
    "PCA is a dimensionality reduction technique used in machine learning. It transforms the original variables into a new set of variables called principal components (PCs). These components are uncorrelated, and each one captures maximum variance.\n",
    "\n",
    "- Reduce dimensions while retaining most information\n",
    "- Visualize high-dimensional data in 2D/3D\n",
    "- Remove multicollinearity\n",
    "\n",
    "---\n",
    "## 🔸 PCA Formulas and Steps\n",
    "### Key Steps:\n",
    "- Standardization (optional) – to bring features to the same scale.\n",
    "- Centering the Data – subtract the mean from each feature.\n",
    "- Covariance Matrix Computation – shows how variables co-vary.\n",
    "- Eigen Decomposition – extract eigenvalues and eigenvectors.\n",
    "- Projection – project data onto principal components.\n",
    "\n",
    "---\n",
    "### Key Formulas:\n",
    "\n",
    "***1. Centering the Data:***\n",
    "$$\n",
    "𝑋_{𝑐𝑒𝑛𝑡𝑒𝑟𝑒𝑑} = X−μ\n",
    "$$\n",
    "Where μ is the mean of each feature.\n",
    "\n",
    "***2. Covariance Matrix:***\n",
    "$$\n",
    "𝐶 = \\frac{1}{n-1}{X_{centered}}^T{X_{centered}}\n",
    "$$\n",
    " \n",
    "***3. Eigen Decomposition:***\n",
    "$$\n",
    "Cv=λv\n",
    "$$\n",
    "Where:\n",
    "- 𝜆: eigenvalue (variance explained)\n",
    "- 𝑣: eigenvector (principal direction)\n",
    "\n",
    "***4. Projection:***\n",
    "$$\n",
    "𝑍 = 𝑋_{𝑐𝑒𝑛𝑡𝑒𝑟𝑒𝑑}⋅𝑊\n",
    "$$\n",
    "Where 𝑊 contains eigenvectors as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef41631-0584-4c0d-aab9-fbc26e2b5691",
   "metadata": {},
   "source": [
    "## 📊 Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b9afc26-1321-40f7-afd1-d4d33683d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9453468d-7f71-40ab-aa17-8fd0c13d26f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Physics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Math  Physics\n",
       "0    90       88\n",
       "1    85       80\n",
       "2    82       78\n",
       "3    70       65\n",
       "4    65       60"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample dataframe for the model input and manual calculatons\n",
    "df = pd.DataFrame({\n",
    "    'Math': [90, 85, 82, 70, 65],\n",
    "    'Physics': [88, 80, 78, 65, 60]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6424262d-f94c-4062-a5c1-0be3d273e54e",
   "metadata": {},
   "source": [
    "## 🔸 Standerdize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ede8c162-0fa3-4606-9a3e-df83981c494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['Math', 'Physics']] = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486554a-2a88-4c19-beac-90249cffe8d2",
   "metadata": {},
   "source": [
    "## 📝 Manual PCA Calculations\n",
    "\n",
    "#### ✅ Step 1: Center the Data\n",
    "We subtract the mean of each feature from the dataset.\n",
    "\n",
    "1. Compute mean of each column (feature):\n",
    "$$\n",
    "μ_1 = \\frac{1.2349 + 0.7026 + 0.3832 − 0.8942 − 1.4265}{5} = 0.0\n",
    "$$\n",
    "$$\n",
    "μ_2 = \\frac{1.3470+0.5661+0.3709−0.8980−1.3860}{5} = 0.0\n",
    "$$\n",
    "\n",
    "So the data is already centered (mean = 0).\n",
    "\n",
    "---\n",
    "#### ✅ Step 2: Compute Covariance Matrix\n",
    "$$\n",
    "𝐶 = \\frac{1}{n-1}{X_{centered}}^T{X_{centered}}\n",
    "$$\n",
    "$$\n",
    "C = \\frac{1}{n - 1} X^T X \\quad \\text{(since the mean is zero)}\n",
    "$$\n",
    "Since the data is centered, we don’t need to subtract the mean again.\n",
    "\n",
    "---\n",
    "#### 📌 Given Matrices\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1.23488137 & 1.3469987 \\\\\n",
    "0.70260492 & 0.56612989 \\\\\n",
    "0.38323905 & 0.37091268 \\\\\n",
    "-0.89422444 & -0.89799913 \\\\\n",
    "-1.4265009 & -1.38604214\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "X^T = \\begin{bmatrix}\n",
    "1.23488137 & 0.70260492 & 0.38323905 & -0.89422444 & -1.4265009 \\\\\n",
    "1.3469987 & 0.56612989 & 0.37091268 & -0.89799913 & -1.38604214\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The result will be a 2×2 matrix:\n",
    "$$\n",
    "C = \\frac{1}{n - 1} X^T X = \\begin{bmatrix}\n",
    "C_{11} & C_{12} \\\\\n",
    "C_{21} & C_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C_{11} = \\frac{1}{n-1}\\sum{x_{i1}^2} = \\frac{1}{4}(1.23488^2 + 0.70260^2 + 0.38323^2 + (−0.89422)^2 + (−1.42650)^2) = \\frac{1}{4}(5.0010) = 1.2503$\n",
    "- $C_{12} = C_{21} = \\frac{1}{n-1}\\sum{x_{i1}x_{i2}} = \\frac{1}{4}(1.2349⋅1.347 + 0.7026⋅0.566 + 0.3832⋅0.3709 + (−0.8942)⋅(−0.898) + (−1.4265)⋅(−1.386)) = \\frac{1}{4}(4.983) = 1.2457$\n",
    "- $C_{22} = \\frac{1}{n-1}\\sum{x_{i2}^2} = \\frac{1}{4}(1.347^2 + 0.566^2 + 0.3709^2 + (−0.898)^2 + (−1.386)^2) = \\frac{1}{4}(5.0006) = 1.2502$\n",
    "\n",
    "#### ✅ Final Covariance Matrix\n",
    "$$\n",
    "Covariance Matrix = \\begin{bmatrix}\n",
    "Cov(𝑋_1,𝑋_1) & Cov(𝑋_1,𝑋_2) \\\\\n",
    "Cov(𝑋_2,𝑋_1) & Cov(𝑋_2,𝑋_2)\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix}\n",
    "1.2503 & 1.2457 \\\\\n",
    "1.2457 & 1.2502\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "---\n",
    "## ✅ Step 4: Find Eigenvalues from Characteristic Polynomial\n",
    "\n",
    "### Covariance matrix:\n",
    "\n",
    "We want to solve:\n",
    "\n",
    "$$\n",
    "\\det(C - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "That gives:\n",
    "$$\n",
    "(1.2503 - \\lambda)(1.2502 - \\lambda) - (1.2457)^2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda^2 - 2.5005\\lambda + (1.56312506 - 1.551779) = 0\n",
    "$$\n",
    "$$\n",
    "\\lambda^2 - 2.5005\\lambda + 0.011346 = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Solve using quadratic formula:\n",
    "\n",
    "$$\n",
    "\\lambda = \\frac{2.5005 \\pm \\sqrt{(-2.5005)^2 - 4(1)(0.011346)}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda = \\frac{2.5005 \\pm \\sqrt{6.2525 - 0.045384}}{2} = \\frac{2.5005 \\pm \\sqrt{6.2071}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\approx \\frac{2.5005 + 2.4914}{2} = 2.496\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_2 \\approx \\frac{2.5005 - 2.4914}{2} = 0.0045\n",
    "$$\n",
    "\n",
    "### ✅ Eigenvalues:\n",
    "\n",
    "- $( \\lambda_1 \\approx 2.496 )$\n",
    "- $( \\lambda_2 \\approx 0.0045 )$\n",
    "\n",
    "---\n",
    "## ✅ Step 5: Find Eigenvectors\n",
    "\n",
    "We solve:\n",
    "\n",
    "$$\n",
    "(C - \\lambda I)\\vec{v} = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### For $( \\lambda_1 = 2.496 )$:\n",
    "\n",
    "$$\n",
    "C - \\lambda_1 I =\n",
    "\\begin{bmatrix}\n",
    "1.2503 - 2.496 & 1.2457 \\\\\n",
    "1.2457 & 1.2502 - 2.496\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "-1.2457 & 1.2457 \\\\\n",
    "1.2457 & -1.2458\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This simplifies to the equation:\n",
    "\n",
    "$$\n",
    "-1.2457x + 1.2457y = 0 \\Rightarrow x = y\n",
    "$$\n",
    "\n",
    "So one eigenvector is:\n",
    "\n",
    "$$\n",
    "\\vec{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ✅ Need to find Unit Eigenvector:\n",
    "\n",
    "1. Calculate the magnitude of the vector:\n",
    "\n",
    "$$\n",
    "\\| \\vec{v}_1 \\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}\n",
    "$$\n",
    "\n",
    "2. Normalize the vector:\n",
    "\n",
    "$$\n",
    "\\vec{v}_{1_{norm}} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0.7071 \\\\ 0.7071 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### For $( \\lambda_2 = 0.0045 )$:\n",
    "\n",
    "$$\n",
    "C - \\lambda_2 I =\n",
    "\\begin{bmatrix}\n",
    "1.2503 - 0.0045 & 1.2457 \\\\\n",
    "1.2457 & 1.2502 - 0.0045\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1.2458 & 1.2457 \\\\\n",
    "1.2457 & 1.2457\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow x = -y\n",
    "$$\n",
    "\n",
    "So one eigenvector is:\n",
    "\n",
    "$$\n",
    "\\vec{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ✅ Need to find Unit Eigenvector\n",
    "1. Calculate the magnitude of the vector:\n",
    "\n",
    "$$\n",
    "\\| \\vec{v}_2 \\| = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}\n",
    "$$\n",
    "\n",
    "Normalize the vector:\n",
    "\n",
    "$$\n",
    "\\vec{v}_{2_{norm}} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0.7071 \\\\ -0.7071 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Unit Eigenvectors:\n",
    "\n",
    "- $( \\vec{v}_{1_{norm}} = \\begin{bmatrix} 0.7071 \\\\ 0.7071 \\end{bmatrix} )$ — Principal Axis 1  \n",
    "- $( \\vec{v}_{2_{norm}} = \\begin{bmatrix} 0.7071 \\\\ -0.7071 \\end{bmatrix} )$ — Principal Axis 2\n",
    " ---\n",
    "## ✅ Step 6: Form the Projection Matrix (Feature Vector Matrix)\n",
    "\n",
    "Take the eigenvectors you calculated and arrange them **column-wise** in a matrix. These are your **principal components**.\n",
    "\n",
    "Let’s say you keep **both components**:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "0.7071 & 0.7071 \\\\\n",
    "0.7071 & -0.7071\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If you only want to keep the **first principal component** (for dimensionality reduction to 1D):\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "0.7071 \\\\\n",
    "0.7071\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step 7: Project the Original Data onto Principal Components\n",
    "\n",
    "Use the formula:\n",
    "\n",
    "$$\n",
    "Z = X \\cdot W\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $( X )$ = Mean-centered original data  \n",
    "- $( W )$ = Matrix of eigenvectors (principal components)  \n",
    "- $( Z )$ = Transformed data in PCA space\n",
    "\n",
    "---\n",
    "### ✅ First Principal Component Projection Values\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix} 0.7071 \\\\ 0.7071 \\end{bmatrix}, \\quad\n",
    "X = \\begin{bmatrix}\n",
    "1.2349 & 1.3470 \\\\\n",
    "0.7026 & 0.5661 \\\\\n",
    "0.3832 & 0.3709 \\\\\n",
    "-0.8942 & -0.8980 \\\\\n",
    "-1.4265 & -1.3860\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute each projection $( z_i = X_i \\cdot W )$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 &= 0.7071 \\cdot 1.2349 + 0.7071 \\cdot 1.3470 = 0.8736 + 0.9523 = \\mathbf{1.8259} \\\\\n",
    "z_2 &= 0.7071 \\cdot 0.7026 + 0.7071 \\cdot 0.5661 = 0.4963 + 0.4004 = \\mathbf{0.8967} \\\\\n",
    "z_3 &= 0.7071 \\cdot 0.3832 + 0.7071 \\cdot 0.3709 = 0.2711 + 0.2624 = \\mathbf{0.5335} \\\\\n",
    "z_4 &= 0.7071 \\cdot (-0.8942) + 0.7071 \\cdot (-0.8980) = -0.6326 - 0.6348 = \\mathbf{-1.2674} \\\\\n",
    "z_5 &= 0.7071 \\cdot (-1.4265) + 0.7071 \\cdot (-1.3860) = -1.0088 - 0.9803 = \\mathbf{-1.9891}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Final Projected Data:\n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "1.8259 \\\\\n",
    "0.8967 \\\\\n",
    "0.5335 \\\\\n",
    "-1.2674 \\\\\n",
    "-1.9891\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16723e9b-5913-4ee6-b10e-0ef7cb2f299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=1)  # Keep only 1 principal component\n",
    "df['first_PC'] = pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "695a159b-5f2d-4c1b-81fc-beb073398b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Principal Component Projection Values (Z):\n",
      "0    1.825665\n",
      "1    0.897131\n",
      "2    0.533266\n",
      "3   -1.267293\n",
      "4   -1.988768\n",
      "Name: first_PC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"First Principal Component Projection Values (Z):\")\n",
    "print(df['first_PC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4180c793-38d0-49c4-ac28-e5f875ab8381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eigenvectors (Principal Components):\n",
      "[[0.70710678 0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Output the components (eigenvectors)\n",
    "print(\"\\nEigenvectors (Principal Components):\")\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0182cc-2d7d-40ec-8761-4be40ed6a213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b105779-c8c6-4da2-9546-aace76796c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14268f-d7ed-45e7-8323-2b893fb242ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
